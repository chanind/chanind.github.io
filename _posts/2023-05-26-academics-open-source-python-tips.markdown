---
layout: post
title: "Academics: You're Doing Open Source Wrong"
date: 2023-05-26
---

<div>
    <img src="/assets/broken_glass_cake.png" />
</div>
<i class="small gray">Broken glass cake, generated by Midjourney</i>

I recently started a PhD in Computer Science after spending the past 10 years working as a software engineer. One of the biggest shocks to me in this transition has been the apalling state of code that's released by academics when they publish research papers. Usually when I complain about academic code, people think I'm just talking about code quality being poor (which it is), but it's much deeper than that. The way code is open-sourced in most academic papers is typically completely broken, and shows a deep misunderstanding about what code is for, and what open-source is about.

Imagine you invite some friends to your apartment, and one of them brings a cake they baked. When you eat the cake, you find that your friend used bits of broken glass as frosting on the cake. Shocked, you point this out to your friend, who just replies, "Oh, I'm not good at cooking, sorry it doesn't taste very good". When you point out that the issue is the broken glass, not the taste, your friend gets upset and tells you that he brought the cake for free, and he didn't have to bring a cake, so you should be grateful. You then realize your friend has a misunderstanding at a fundamental level about what cooking is for, and what food even is. The code academics release is like that - poorly made cake full of broken glass shards.

In this post, I'll go over what I view as the problem, and share tips for academics on how to do a better job of open-sourcing their code. I'll be focusing on Python, as that's mainly what's used in AI research, but a lot of this will apply to other languages as well. First and most importantly, we'll remove the broken glass shards from the code, and then we'll go over how to improve the taste, aka code quality.

## The problematic academic mindset

Most code I encounter that's released as part of academic papers is completely broken, as in it's not possible to run the code as provided at all. This is typically due to things like missing files the researcher forgot to upload, or hardcoded file paths to stuff on the researcher's own machine, missing documentation, or not pining Python dependency versions. This shows that the researcher never even tried running the code they open-sourced at all, and instead just copied and pasted some files from their local hard-drive into a Github repo, linked the repo in their paper, and high-fived everyone for a job well done. There is no notion that other people are going want to actually try to run that code, and that by uploading broken code and advertising it in a paper you are directly wasting thousands of hours of other people's time.

Academics are not bad people, and I don't believe they're intentionally being malicious. Instead, I think the mindset of most researchers towards open-source code is the following:

- Putting Python files in a Github repo is just a way to make a paper seem more legit, since if there's code then people will believe the results more
- Code is just for looking at to get an idea of how something is implemented, not for running

This is not just cruel to the people who try to use code written by academics, but also a huge missed opportunity. The times when academic code is open-sourced well and is packaged as an easy-to-use library, the corresponding paper gets tons of citations as other researchers use the library in their own work. Doing a decent job open-sourcing code is easy, as you'll see below, and the potential rewards to the researcher are immense.

<div>
    <img src="/assets/drake_install_software.jpeg" />
</div>
<i class="small gray">Python libraries should be packaged and released on PyPI. A collection of broken Python scripts on Github != software.</i>

## Release a library, not a collection of files

The core issue with the academic mindset to open-sourcing code is that it completely misses what people want to do with code after reading a paper. Your code should focus on actually doing what your paper is about, not just reproducing the results in the paper. Of course, there's nothing wrong with including the code that reproduces your results, it just should not be the main focus. Ideally, your goal should be to release a _library_ which does the thing in your paper, not a pile of random Python files.

For instance, imagine you need to do dog picture classification, and after some searching you find that the state-of-the-art for dog classification is a technique called "DogBert". After reading the paper, you find they open-sourced their code. "Great!, I'll just used that," you think. However, when you get to the DogBert github repo, you find that you need to clone their git repo, and they have some Python scripts which just reproduce the numbers in their paper. "Ugh, well, maybe I can repurpose this", you think. You then struggle with it for several hours, eventually discovering the author forgot to include some files, and probably used different versions of dependencies that aren't specified. Frustrated, you eventually give up and just use another library that's not as high performance, but at least it works. DogBert's broken code has just caused you to waste a whole day, and you're not happy.

Now imagine, instead, that when you get to the DogBert git repo, you find you can install the library with `pip install dogbert`. Then, to use it you just run:

```python
from dogbert import dogbert_classify_img

dog_type = dogbert_classify_img('/path/to/img')
```

You install the library, use that code, and you're happily classifying dog images with the pretrained dogbert model in no time. In addition, when you publish a paper on your work, you'll cite DogBert since you're using it in your code. Everybody wins!

As a researcher publishing your code, at a minimum you should do the following:

- Release your code as a library on PyPI (this makes it installable with `pip install`)
- Have a basic README explaining how to use the library
- The library should be usable in Python, it shouldn't require running a Python script
- Manually test your code and instructions to make sure it works as advertised
- If people use your code and find bugs, you should respond and fix the bugs

The idea of releasing a library rather than just copy/pasting Python files into Github might sound daunting, but it's really not difficult. The difference is mostly a code organization question more than anything else, and some basic thought put to "what would someone want to do with this code?". Once you've learned to package code into a library you'll see that doing a decent job of open-sourcing your code is _easy_ - it's far easier than learning LaTeX, or writng a paper, or finding a research idea to begin with. We'll discuss how to make this easy in the section on Poetry below.

## Splitting code into multiple repos / libraries

If your paper has several distinct components, each of which could be used independently as its own library, there's nothing wrong with releasing multiple open-source repos or libraries along with your paper. The goal of open-sourcing code should be a focus on how it's most useful to others, and you might find that it's more natural to release 2 or even 3 different libraries for the various parts of your paper rather than trying to fit everything into 1 library. There's no rule that says every paper must correspond to 1 and only 1 git repo.

## What about reproducing the results in my paper? Shouldn't that be the main point of open-sourced code?

By all means, do include code to reproduce the experiments in your paper. However, recognize that likely nobody is going to actually run this code or even look at it, so it shouldn't be the main focus. It's fine to include an `experiments` folder in your git repo that's not published to PyPI, or even split apart the experiments into a separate git repo from the reusable library code so the library can evolve separately. If you take the splitting the repos approach, then the experiments repo can include the library as a normal `pip` dependency, which also has the bonus of verifying that your library works when installed as a dependency in other projects.

## What are some examples of this done well?

The best examples of researchers releasing their code and packaging it well are also some of the best known projects in the field. I don't believe this is a coincidence - if you package your code properly and release it on PyPI, then other will use it in their own projects and cite your paper. Two excellent examples that come to mind are the following:

#### FlashAttention

[FlashAttention](https://github.com/HazyResearch/flash-attention) is a beautiful illustration of how you don't need to overthink this to do a good job. This repo has a simple README with installation of the library via `pip install flash-attn` and basic instructions on how to use it in Python. There's a `benchmarks` folder in the repo to reproduce the results in the paper, but it's not the main focus of the library. The library itself is simple and focused. A+

#### Sentence Transformers

[Sentence Transformers](https://github.com/UKPLab/sentence-transformers) goes above and beyond, including a documentation website and continues to evolve and add pretrained models to the library. This library corresponds to an original paper by the author on a technique for sentence similarity, but was just packaged well and focused on ease of use, and the author clearly has put a lot of care into this library.

Both of these libraries were created by individual researchers along with their papers, by a PhD student in the case of FlashAttention, and a postdoc in the case of Sentence Transformers. In both these cases, the authors could have just copy/pasted a collection of unusable Python scripts into a Github repo and left it at that, but then likely neither of their papers would have achieved anywhere near the level of success they both have seen. I believe the level of polish that these libraries show is very achievable for all academics, and should be the norm rather than the exception.

## Packaging and dependency management made easy: Python-poetry

Personally, I like using [Poetry](https://python-poetry.org/) for managing Python projects. Poetry handles a lot of the complexity of virtual environments for Python, dependency management, and finally, publishing your library to PyPI so it can be installed with `pip install <your-library>`. Poetry isn't the only way to do this, but it provides a good foundation.

Let's assume we're the authors of the DogBert paper about dog image classification. We could start by making a new Poetry project:

```bash
poetry new dogbert
```

This will give us the following file structure:

```
dogbert
├── README.md
├── dogbert
│   └── __init__.py
├── pyproject.toml
└── tests
    └── __init__.py
```

It may seem confusing that there's 2 nested folders, both named `dogbert`, but this is a standard setup for Python projects. The inner `dogbert` folder containing `__init__.py` is where all our library Python files will go. If you write tests (and you should!), those go in the `tests` folder.

We `cd` into the outer `dogbert` folder, and run `poetry install` to initialize a new pyenv environment for our project, and make sure any needed dependencies are installed.

We can add any pip dependencies our project needs with `poetry add <dependency>`. Finally, when we want to publish our library on PyPI so it can be installed with `pip install dogbert`, we just run the following 2 commands:

```bash
poetry build
poetry publish
```

And that's it, our library is on PyPI! There's really not much to it, that's all it takes to package and publish a library to PyPI.

#### Imports and scripts in Poetry

If you're used to just writing standalone python scripts in a single file and running them with `python my_file.py`, Poetry might seem strange at first. If we have file `utils.py` at `dogbert/utils.py` with a function called `preprocess()`, and we have have another file which wants to import that `preprocess` function, we can import it like below:

```python
from dogbert.utils import preprocess
```

Poetry creates its own pyenv enviroment so different projects have independent sets of installed Python modules. This is great, but it means that instead of directly running `python`, you need to prefix all commands on the CLI with `poetry run` so the correct pyenv is used. Also, for scripts, it's best to run them using python's module flag. Where you may be used to directly running a script file with `python path/to/script.py`, when using Poetry you'd instead run `poetry run python -m path.to.script`. If we had a script called `train.py` at `dogbert/scripts/train.py`, we could run that with `poetry run python -m dogbert.scripts.train`.

This may take some getting used-to initially, but it's a minor workflow change which quickly becomes second-nature.

#### Aside: Poetry with Pytorch

I've had issues in the past with adding Pytorch as a dependency from Poetry since Pytorch has multiple versions with different CUDA requirements, which Poetry doesn't handle well. I find it's best to simply leave it to the end-user of your library to install Pytorch, and not try to force it via `poetry add torch`, since it's easy to end up with a non-CUDA version of pytorch that way. Oftentimes, if you're relying on libraries like `pytorch-lightning` or other popular machine learning libraries, they'll already handle making sure PyTorch is installed. If you want to include torch as a dependency, I'd recommend adding it as a dev dependency `poetry add --group dev torch` so you won't accidentally end up with a CPU-only version of PyTorch being installed for end-users of your library. Hopefully this will be handled better in future versions of Poetry/PyTorch!

## Use a linter

While I don't feel like code quality is nearly as important as proper Python packaging and checking that your published code actually works, code quality is still important. Linters like [Flake8](https://flake8.pycqa.org/en/latest/) or [Pylint](https://pylint.readthedocs.io/en/latest/) will check your code for common code-quality issues like unused variables and report them as errors. All popular code editors have plugins for Python linters which will highlight linting errors directly in your code. You can also customize the errors the linters report if there are types of errors you want to ignore. Linting errors ofter correspond to real bugs in your code, and are an easy way to improve your code quality at almost no cost. There's really no downside to using a linter.

Related to linters are code formatters like [Black](https://black.readthedocs.io/en/stable/). Black will automatically format your code for you so the formatting is consistent, and takes away the need for you to think about formatting entirely. Personally, I think code formatters are great and recommend using Black, but this isn't a universal opinion in the Python world. I'd recommend experimenting with this and see if you like it. There are plugins for all code editors which will let you automatically run Black whenever you save a Python file, which makes it really seamless.

## Writing tests

Testing is something that I didn't understand the value of until I started working professionally. There's a natural aversion to writing tests as it feels like a bunch of extra work you need to do, and everyone always feels like they don't have time for that. However, as I've improved as a software engineer and gotten more comfortable with writing tests, I find the exact opposite: I don't have time **not** to write tests.

If you don't add test cases as you code, you're probably testing manually. However, this manual testing means that anytime you want to make a change to your existing code, either to refactor or to add new features, you're always terrified you might accidentally break something you already wrote. Then you need to either go back and manually test everything again, likely forgetting something, or you just hack in your change in whatever way is the least likely to break something. This "fear-driven development" leads to horrible messes of code that are almost certainly broken and
